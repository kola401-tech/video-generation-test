# ============================================
# üé• Notebook Colab : G√©n√©ration Vid√©o Hugging Face
# ============================================

# SECTION 1 : Installer les d√©pendances
!pip install diffusers transformers accelerate safetensors imageio[ffmpeg]

import torch
import imageio
from PIL import Image
import requests

# ============================================
# SECTION 2 : Image ‚Üí Vid√©o (Stable Video Diffusion)
# ============================================
from diffusers import StableVideoDiffusionPipeline

print("üöÄ Chargement du mod√®le Stable Video Diffusion...")
model_img2vid = "stabilityai/stable-video-diffusion-img2vid-xt"
pipe_img2vid = StableVideoDiffusionPipeline.from_pretrained(
    model_img2vid, torch_dtype=torch.float16, variant="fp16"
)
pipe_img2vid = pipe_img2vid.to("cuda")

# Exemple : charger une image depuis le web
url = "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/img2img/sketch-mountains-input.png"
image = Image.open(requests.get(url, stream=True).raw)

# G√©n√©rer une courte vid√©o (14 frames)
generator = torch.manual_seed(42)
frames_img2vid = pipe_img2vid(
    image,
    num_frames=14,
    motion_bucket_id=127,
    noise_aug_strength=0.1,
    generator=generator
).frames[0]

# Sauvegarder en MP4
video_path_img2vid = "video_from_image.mp4"
imageio.mimsave(video_path_img2vid, frames_img2vid, fps=6)
print("‚úÖ Vid√©o g√©n√©r√©e √† partir d'une image :", video_path_img2vid)


# ============================================
# SECTION 3 : Texte ‚Üí Vid√©o (Zeroscope v2)
# ============================================
from diffusers import DiffusionPipeline

print("üöÄ Chargement du mod√®le Zeroscope v2...")
model_text2vid = "cerspense/zeroscope_v2_576w"
pipe_text2vid = DiffusionPipeline.from_pretrained(model_text2vid, torch_dtype=torch.float16)
pipe_text2vid = pipe_text2vid.to("cuda")

# Exemple de prompt
prompt = "Un petit robot qui danse dans une pi√®ce futuriste, style cin√©matique, ultra r√©aliste."

# G√©n√©rer une vid√©o de 24 frames (~4 secondes)
generator = torch.manual_seed(1234)
frames_text2vid = pipe_text2vid(
    prompt,
    num_frames=24,
    guidance_scale=12.5,
    generator=generator
).frames[0]

# Sauvegarder en MP4
video_path_text2vid = "video_from_text.mp4"
imageio.mimsave(video_path_text2vid, frames_text2vid, fps=12)
print("‚úÖ Vid√©o g√©n√©r√©e √† partir d'un texte :", video_path_text2vid)


# ============================================
# SECTION 4 : R√©sultats
# ============================================
from IPython.display import Video

print("\nüé¨ Aper√ßu Vid√©o Image ‚Üí Vid√©o")
display(Video(video_path_img2vid, embed=True))

print("\nüé¨ Aper√ßu Vid√©o Texte ‚Üí Vid√©o")
display(Video(video_path_text2vid, embed=True))
